# Configuration file for a CARLA simulation with:
# - one ego vehicle drives straight south through intersection
# - one stopped sensor-capturing stopped at north side of intersection
# - one driving sensor-capturing vehicle that drives straight west through 

# ---+ GENERAL CONFIGURATION +---
# This section contains general settings for the simulation.
#
# name: must be "general"
# seed: the random seed for the simulation, used for reproducibility, or None
#   for a random seed. This affects where vehicles are spawned, which type is
#   spawned, where they drive, and traffic manager behavior (stop lights, etc.).
# n_ticks: the number of ticks to run the simulation for. This defines the 
#   length of the simulation. Note that 20 ticks correspond to one real-life 
#   second.
# output_dir: the directory where the simulation output will be saved. This
#   directory will be created if it does not exist. The output will include
#   sensor data and ground truth poses for each sensor. The directory is 
#   relative to the project root, which is the same directory as "README.md".
#   Data for each sensor will be stored in "<output_dir>/<sensor_name>", 
#   example: "build/sim_0_output/ego_lidar"

[general]
# seed = 128
seed = 314
n_ticks = 256
output_dir = ./build/sim_3_output


# ---+ AGENTS +---
# Agents are vehicles that are spawned into the simulation, and can be
#   configured to be either controlled by the autopilot or manually. Z, pitch,
#   and roll are set automatically by transforming to the nearest valid spawn.
#   These values are not used in the simulation, but are required for the config 
#   to be valid.
# 
# name: needs to end in "_vehicle" to be recognized
# location: the location of the vehicle in the world, relative to the world 
#   origin, given as x, y, z. 
# rotation: the rotation of the vehicle in the world, relative to the world 
#   origin, given as pitch, yaw, roll.
# filter: the filter of the vehicle to spawn, example: vehicle.dodge.charger
# type: the type of the vehicle, example: car, motorcycle, bicycle
# autopilot: whether the vehicle is controlled by the autopilot or manually 
#   (True or False)

[ego_vehicle]
# Facing south in left lane, makes left at intersection
location = -27, 16.0, 0.0
rotation = 0.0, 180.0, 0.0
filter = vehicle.dodge.charger
type = car
autopilot = True

[agent_0_vehicle]
# Facing north in left lane, makes left at intersection
location = -69.5, 24.0, 0.0
rotation = 0.0, 0.0, 0.0
filter = vehicle.dodge.charger
type = car
autopilot = True

[agent_1_vehicle]
# Facing west in right lane, stopped at intersection
location = -45.0, 44.0, 0.0
rotation = 0.0, -90.0, 0.0
filter = vehicle.dodge.charger
type = car
autopilot = False


# ---+ OTHER VEHICLES +---
# Other vehicles are spawned into the simulation, but do not have a specific
#   location. They are spawned randomly in the world, and can be configured to
#   have a specific filter, type, and number of vehicles to spawn.
[other_vehicles]
n_vehicles = 16
filter = vehicle.*.*
type = car


# ---+ SENSORS +---
# Sensors are attached to vehicles and collect data during the simulation.
#   output data is saved to <output_dir>/<sensor_name>, and includes a "frames"
#   directory with images or point clouds and a "ground_truth_poses_tum.txt"
#   file with the poses of the sensor at each respective frame.
#
# name: needs to end in "_lidar" or "_camera" to be recognized
# attach_to: the exact name of the vehicle to attach the sensor to (example: 
#   ego_vehicle) or None if the sensor is not attached to any vehicle
# location: the location of the sensor relative to the vehicle it is attached
#   to, or relative to world origin if attach_to is None, given as x, y, z. Note
#   that if relative to vehicle, +X is forward, +Y is right, and +Z is up
# rotation: the rotation of the sensor relative to the vehicle it is attached
#   to, or relative to world origin if attach_to is None

# Single ego cam only for visualization purposes
[ego_camera]
attach_to = ego_vehicle
# Camera 0.27m forward and 0.05 down of LiDAR (as in KITTI setup)
location = 0.27, 0.0, 1.65
rotation = 0.0, 0.0, 0.0

[ego_lidar]
attach_to = ego_vehicle
location = 0.0, 0.0, 1.73
rotation = 0.0, 0.0, 0.0

[agent_0_lidar]
attach_to = agent_0_vehicle
location = 0.0, 0.0, 1.73
rotation = 0.0, 0.0, 0.0

[agent_1_lidar]
attach_to = agent_1_vehicle
location = 0.0, 0.0, 1.73
rotation = 0.0, 0.0, 0.0

# North east corner
# TODO: fix dis
[ne_lidar]
location = -34.0, 36.0, 5.5
data_dir = ne_lidar

# South west corner
[sw_lidar]
location = -61.5, 7.5, 5.5
data_dir = sw_lidar

# South east corner
[se_lidar]
location = -61.5, 36.0, 5.5
data_dir = se_lidar

# North west corner
# NOTE: NW.x = NE.x, NW.y = SW.y
[nw_lidar]
location = -34.0, 7.5, 5.5
data_dir = nw_lidar


# ---+ DIAGRAM (EXTRA) +---
# Just a little help for visualizing the lidar sensor locations in the default
# CARLA UE5 0.10.0 map.
# - top down view of intersection
# - into the screen is -Z
# - +X is arbitrarily chosen to be north, and +Y to be east
# - (X and Y are flipped because CARLA uses a left-handed coordinate system)
#
#      <courthouse/museum>
#         NW --------- NE
#         |             |
#         |             |
#         |             |
#      +X |             | 
#       ^ |             |
#       | SW --------- SE
#       +---> +Y
# <sculpture>
# 
#    ^
#    | yaw 0 deg
# 
#   ---> yaw  90 deg
#
# ---+ END DIAGRAM +---
